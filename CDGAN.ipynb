{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e28fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout, Input, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from numpy import asarray, savetxt\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from PIL import Image as im\n",
    "from numpy import asarray, savetxt\n",
    "from torch import nn, optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0218d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir_name = 'C:/'\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"Using Device:\", DEVICE)\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "# Data loader\n",
    "\n",
    "trans = transforms.Compose([transforms.Grayscale(), transforms.Resize((280,280)), transforms.ToTensor(),transforms.Normalize(0.5, 0.5)])\n",
    "image = torchvision.datasets.ImageFolder(root=dir_name,transform=trans)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=image,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "labels_map = {0:\"best\", \n",
    "             1:\"normal\",\n",
    "             2:\"worst\",\n",
    "             }\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 1\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(data_loader), size=(1,)).item()\n",
    "    img, label = image[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Hyper-parameters & Variables setting\n",
    "\n",
    "num_epoch = 500\n",
    "learning_rate = 0.0001\n",
    "\n",
    "img_size = 280 * 280\n",
    "num_channel = 3\n",
    "dir_name = \"CDGAN\"\n",
    "noise_size = 3\n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 256\n",
    "hidden_size3 = 512\n",
    "hidden_size4 = 1024\n",
    "\n",
    "\n",
    "condition_size = 3\n",
    "\n",
    "\n",
    "# Device setting\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Now using {} devices\".format(device))\n",
    "\n",
    "\n",
    "# Create a directory for saving samples\n",
    "if not os.path.exists(dir_name):\n",
    "    os.makedirs(dir_name)\n",
    "\n",
    "# Define discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(img_size + condition_size, hidden_size4)\n",
    "        self.linear2 = nn.Linear(hidden_size4, hidden_size3)\n",
    "        self.linear3 = nn.Linear(hidden_size3, hidden_size2)\n",
    "        self.linear4 = nn.Linear(hidden_size2, hidden_size1)\n",
    "        self.linear5 = nn.Linear(hidden_size1, 3)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.linear1(x))\n",
    "        x = self.leaky_relu(self.linear2(x))\n",
    "        x = self.leaky_relu(self.linear3(x))\n",
    "        x = self.leaky_relu(self.linear4(x))\n",
    "        x = self.linear5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "# Define generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(noise_size + condition_size, hidden_size1)\n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.linear3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.linear4 = nn.Linear(hidden_size3, hidden_size4)\n",
    "        self.linear5 = nn.Linear(hidden_size4, img_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        x = self.relu(self.linear4(x))\n",
    "        x = self.linear5(x)\n",
    "        x = self.tanh(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# For checking validity in final step\n",
    "def check_condition(_generator):\n",
    "    test_image = torch.empty(0).to(device)\n",
    "\n",
    "    for i in range(3):\n",
    "        test_label = torch.tensor([0])\n",
    "        test_label_encoded = F.one_hot(test_label, num_classes=3).to(device)\n",
    "\n",
    "        # create noise(latent vector) 'z'\n",
    "        _z = torch.randn(1, noise_size).to(device)\n",
    "        _z_concat = torch.cat((_z, test_label_encoded), 1)\n",
    "\n",
    "        test_image = torch.cat((test_image, _generator(_z_concat)), 0)\n",
    "\n",
    "    _result = test_image.reshape(1, 3, 280, 280)\n",
    "    save_image(_result, os.path.join(dir_name, 'result.png'), nrow=1)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize generator/Discriminator\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "\n",
    "# Device setting\n",
    "discriminator = discriminator.to(device)\n",
    "generator = generator.to(device)\n",
    "\n",
    "# Loss function & Optimizer setting\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Training part\n",
    "\"\"\"\n",
    "for epoch in range(num_epoch):\n",
    "    for i, (images, label) in enumerate(data_loader):\n",
    "\n",
    "        # make ground truth (labels) -> 1 for real, 0 for fake\n",
    "        real_label = torch.full((batch_size, 3), 1, dtype=torch.float32).to(device)\n",
    "        fake_label = torch.full((batch_size, 3), 0, dtype=torch.float32).to(device)\n",
    "\n",
    "        # reshape real images \n",
    "        real_images = images.reshape(batch_size, -1).to(device)\n",
    "\n",
    "       \n",
    "        label_encoded = F.one_hot(label, num_classes=3).to(device)\n",
    "        # concat real images with 'label encoded vector'\n",
    "        real_images_concat = torch.cat((real_images, label_encoded), 1)\n",
    "\n",
    "        # +---------------------+\n",
    "        # |   train Generator   |\n",
    "        # +---------------------+\n",
    "\n",
    "        # Initialize grad\n",
    "        g_optimizer.zero_grad()\n",
    "        d_optimizer.zero_grad()\n",
    "\n",
    "        # make fake images with generator & noise vector 'z'\n",
    "        z = torch.randn(batch_size, noise_size).to(device)\n",
    "\n",
    "        # concat noise vector z with encoded labels\n",
    "        z_concat = torch.cat((z, label_encoded), 1)\n",
    "        fake_images = generator(z_concat)\n",
    "        fake_images_concat = torch.cat((fake_images, label_encoded), 1)\n",
    "\n",
    "        # Compare result of discriminator with fake images & real labels\n",
    "        # If generator deceives discriminator, g_loss will decrease\n",
    "        g_loss = criterion(discriminator(fake_images_concat), real_label)\n",
    "\n",
    "        # Train generator with backpropagation\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # +---------------------+\n",
    "        # | train Discriminator |\n",
    "        # +---------------------+\n",
    "\n",
    "        # Initialize grad\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "\n",
    "        # make fake images with generator & noise vector 'z'\n",
    "        z = torch.randn(batch_size, noise_size).to(device)\n",
    "\n",
    "        # concat noise vector z with encoded labels\n",
    "        z_concat = torch.cat((z, label_encoded), 1)\n",
    "        fake_images = generator(z_concat)\n",
    "        fake_images_concat = torch.cat((fake_images, label_encoded), 1)\n",
    "\n",
    "        # Calculate fake & real loss with generated images above & real images\n",
    "        fake_loss = criterion(discriminator(fake_images_concat), fake_label)\n",
    "        real_loss = criterion(discriminator(real_images_concat), real_label)\n",
    "        d_loss = (fake_loss + real_loss) / 2\n",
    "\n",
    "        # Train discriminator with backpropagation\n",
    "        # In this part, we don't train generator\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        d_performance = discriminator(real_images_concat).mean()\n",
    "        g_performance = discriminator(fake_images_concat).mean()\n",
    "\n",
    "        if (i + 1) % 150 == 0:\n",
    "            print(\"Epoch [ {}/{} ]  Step [ {}/{} ]  d_loss : {:.5f}  g_loss : {:.5f}\"\n",
    "                  .format(epoch + 1, num_epoch, i+1, len(data_loader), d_loss.item(), g_loss.item()))\n",
    "\n",
    "    # print discriminator & generator's performance\n",
    "    print(\" Epock {}'s discriminator performance : {:.2f}  generator performance : {:.2f}\"\n",
    "          .format(epoch + 1, d_performance, g_performance))\n",
    "\n",
    "    # Save fake images in each epoch\n",
    "    \n",
    "    label = label.tolist()\n",
    "    label = label[:3]\n",
    "    label = [str(l) for l in label]\n",
    "    label_text = \", \".join(label)\n",
    "    \n",
    "    samples = fake_images.reshape(batch_size, 1, 280, 280)\n",
    "    save_image(samples, os.path.join(dir_name, 'fake_samples{}({}).png'.format(epoch + 1,label_text)))\n",
    "\n",
    "check_condition(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bdbd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_y = pd.read_csv(\".csv\")\n",
    "\n",
    "target_x = pd.read_csv(\".csv\", usecols=['x1','x2','x3','x4','x6','y_1','y_2'])\n",
    "\n",
    "target_x\n",
    "\n",
    "\n",
    "# normalize dataset with MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 255))\n",
    "target_x = pd.DataFrame(scaler.fit_transform(target_x))\n",
    "target_x\n",
    "target_y=target_y.drop('Unnamed: 0',axis=1)\n",
    "target_y\n",
    "\n",
    "target_y_np = target_y.values\n",
    "target_x_np = target_x.values\n",
    "\n",
    "target_y_np_reshape = target_y_np.reshape(target_y_np.shape[0],-1,280,280)\n",
    "target_x_np_reshape = target_x_np.reshape(target_x_np.shape[0],-1,1,7)\n",
    "print(target_y_np_reshape.shape)\n",
    "print(target_x_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(6,1,280, 280)\n",
    "y = torch.rand(6,7)\n",
    "\n",
    "    \n",
    "class TensorData(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = torch.FloatTensor(x_data)\n",
    "        self.y_data = torch.FloatTensor(y_data)\n",
    "        self.len = self.y_data.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "trainsets = TensorData(x, y)\n",
    "trainloader = DataLoader(trainsets, batch_size = 6, shuffle = True)\n",
    "print(trainloader)\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(10,10), stride=2, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(10,10), stride=2, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32,  kernel_size=(10,10), stride=2, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64,  kernel_size=(10,10), stride=2, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=1,  kernel_size=(10,10), stride=2, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.fc1 = nn.Linear(1,136,bias=False)\n",
    "        self.fc2 = nn.Linear(136,64,bias=False)\n",
    "        self.fc3 = nn.Linear(64,28,bias=False)\n",
    "        self.fc4 = nn.Linear(28,10,bias=False)\n",
    "        self.fc5 = nn.Linear(10,7,bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "#               \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def tt_split(X_1,Y_1):\n",
    "    \n",
    "    \n",
    "    sc_X = StandardScaler()\n",
    "    sc_Y = StandardScaler()\n",
    "    \n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_1,Y_1,test_size=0.2)\n",
    "    \n",
    "    trainsets = TensorData(X_train, Y_train)\n",
    "    trainloader = DataLoader(trainsets, batch_size = 6, shuffle = True)\n",
    "    testsets = TensorData(X_test, Y_test)\n",
    "    testloader = DataLoader(testsets, batch_size = 6, shuffle = False)\n",
    "    \n",
    "    return trainloader, testloader, sc_X, sc_Y\n",
    "\n",
    "\n",
    "\n",
    "def evaluation(dataloader, model, sc_X, sc_Y):\n",
    "    predictions = torch.tensor([], dtype=torch.float)\n",
    "    actual = torch.tensor([],dtype=torch.float)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in dataloader:\n",
    "            inputs, values = data\n",
    "            outputs = model(inputs)\n",
    "            predictions = torch.cat((predictions, outputs), 0)\n",
    "            actual = torch.cat((actual, values), 0)\n",
    "    \n",
    "    predictions = predictions.numpy()\n",
    "    actual = actual.numpy()\n",
    "\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(predictions.reshape(predictions.shape[0],7), actual.reshape(actual.shape[0],7)))\n",
    "        \n",
    "    print(predictions)\n",
    "    print(actual)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MLP(X,Y):\n",
    "    model = MultiLayerPerceptron()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 1e-4)\n",
    "    \n",
    "    trainloader, testloader, sc_X, sc_Y = tt_split(X,Y)\n",
    "\n",
    "    loss_ = []\n",
    "    n = len(trainloader)\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        running_loss = 0.0\n",
    "        for data in trainloader:\n",
    "            inputs, values, = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, values)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        loss_.append(running_loss/n)\n",
    "        print(\" Epock {} loss {}\".format(epoch + 1, loss))\n",
    "        \n",
    "    plt.plot(loss_)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    \n",
    "    train_rmse = evaluation(trainloader, model, sc_X, sc_Y)\n",
    "    test_rmse = evaluation(testloader, model, sc_X, sc_Y)\n",
    "    print('Train RMSE: ', train_rmse)\n",
    "    print('Test RMSE: ', test_rmse)\n",
    "\n",
    "    \n",
    "    torch.save(model, 'C:/.pt')\n",
    "\n",
    "MLP(target_y_np_reshape, target_x_np_reshape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8be3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining results statistically\n",
    "\n",
    "model = torch.load('C:/.pt')\n",
    "\n",
    "#Signal = np.load('Augmented_signal.npy')\n",
    "test_y = pd.read_csv(\"all.csv\")\n",
    "test_y=test_y.drop('Unnamed: 0',axis=1)\n",
    "test_y_np = test_y.values\n",
    "test_y_np_reshape = test_y_np.reshape(test_y_np.shape[0],-1,280,280)\n",
    "print(test_y_np_reshape.shape)\n",
    "model.eval()\n",
    "predictions = torch.tensor([], dtype=torch.float)\n",
    "actual = torch.tensor([],dtype=torch.float)\n",
    "sc_Y = StandardScaler()\n",
    "testsets = torch.FloatTensor(test_y_np_reshape)\n",
    "testloader = DataLoader(testsets, batch_size = 1, shuffle = False)\n",
    "\n",
    "for data in trainloader:\n",
    "    inputs, values = data\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs = data\n",
    "        outputs = model(inputs)\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "        actual = torch.cat((actual, values), 0)\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "actual = actual.numpy()\n",
    "a= predictions.T\n",
    "\n",
    "df1 = pd.DataFrame(a[0][0][0],columns = ['x1'])\n",
    "df2 = pd.DataFrame(a[1][0][0],columns = ['x2'])\n",
    "df3 = pd.DataFrame(a[2][0][0],columns = ['x3'])\n",
    "df4 = pd.DataFrame(a[3][0][0],columns = ['x4'])\n",
    "df5 = pd.DataFrame(a[4][0][0],columns = ['x5'])\n",
    "df6 = pd.DataFrame(a[4][0][0],columns = ['y2'])\n",
    "df7 = pd.DataFrame(a[4][0][0],columns = ['y1'])\n",
    "\n",
    "fin_df = pd.concat([df1,df2,df3,df4,df5,df6,df7],axis=1)\n",
    "new = scaler.inverse_transform(fin_df)\n",
    "new\n",
    "df = pd.DataFrame(new)\n",
    "df.to_csv('./var.csv')\n",
    "print(\"Mean=\", new.mean(axis=0))\n",
    "print(\"Max=\", new.max(axis=0))\n",
    "print(\"Min=\", new.min(axis=0))\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef32335e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
